## 📁 llm-studies

### 📂 theory (Teorik bilgiler ve açıklamalar)

- **[LLM (Large Language Model) Nedir?](https://github.com/tahabugracck/llm_studies/tree/main/theory/01_what_is_llm)**

  - Geleneksel NLP Modelleri vs. LLM
  - LLM’lerin Kullanım Alanları
  - LLM’lerin Sınırlamaları ve Etik Sorunlar

- **[Tokenization Nedir?](https://github.com/tahabugracck/llm_studies/tree/main/theory/02_tokenization)**

  - Word-Level, Subword, Character-Level Tokenization
  - Byte-Pair Encoding (BPE)
  - WordPiece & SentencePiece

- **[Embedding Nedir?](https://github.com/tahabugracck/llm_studies/tree/main/theory/03_embeddings)**

  - Word2Vec, GloVe, FastText
  - Transformer Tabanlı Embeddings (BERT, GPT)
  - Vektörel Uzayda Anlam Benzerliği

- **[Transformer Modelinin Genel Yapısı](https://github.com/tahabugracck/llm_studies/tree/main/theory/04_transformer_architecture)**

  - Self-Attention Mekanizması
  - Multi-Head Attention
  - Positional Encoding
  - Encoder-Decoder Yapısı

- **[Vektör Uzayında Veri Temsili](https://github.com/tahabugracck/llm_studies/tree/main/theory/05_vector_representation)**

  - Kelime Vektörleri Nasıl Görselleştirilir?
  - Vektör Uzayında Veri Temsili
  - PCA ve t-SNE Kullanımı

### 📂 implementations (Küçük çaplı kodlar ve örnek uygulamalar)

- `tokenization_example.py` → Tokenization nasıl çalışır?
- `word_embedding_visualization.py` → Embedding’leri görselleştirme
- `self_attention_demo.py` → Self-Attention hesaplaması
- `transformer_from_scratch.py` → Basit Transformer Modeli

### 📄 README.md&#x20;



