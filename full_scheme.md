## ğŸ“ llm-studies

### ğŸ“‚ theory (Teorik bilgiler ve aÃ§Ä±klamalar)

- **[LLM (Large Language Model) Nedir?](https://github.com/tahabugracck/llm_studies/tree/main/theory/01_what_is_llm)**

  - Geleneksel NLP Modelleri vs. LLM
  - LLMâ€™lerin KullanÄ±m AlanlarÄ±
  - LLMâ€™lerin SÄ±nÄ±rlamalarÄ± ve Etik Sorunlar

- **[Tokenization Nedir?](https://github.com/tahabugracck/llm_studies/tree/main/theory/02_tokenization)**

  - Word-Level, Subword, Character-Level Tokenization
  - Byte-Pair Encoding (BPE)
  - WordPiece & SentencePiece

- **[Embedding Nedir?](https://github.com/tahabugracck/llm_studies/tree/main/theory/03_embeddings)**

  - Word2Vec, GloVe, FastText
  - Transformer TabanlÄ± Embeddings (BERT, GPT)
  - VektÃ¶rel Uzayda Anlam BenzerliÄŸi

- **[Transformer Modelinin Genel YapÄ±sÄ±](https://github.com/tahabugracck/llm_studies/tree/main/theory/04_transformer_architecture)**

  - Self-Attention MekanizmasÄ±
  - Multi-Head Attention
  - Positional Encoding
  - Encoder-Decoder YapÄ±sÄ±

- **[VektÃ¶r UzayÄ±nda Veri Temsili](https://github.com/tahabugracck/llm_studies/tree/main/theory/05_vector_representation)**

  - Kelime VektÃ¶rleri NasÄ±l GÃ¶rselleÅŸtirilir?
  - VektÃ¶r UzayÄ±nda Veri Temsili
  - PCA ve t-SNE KullanÄ±mÄ±

### ğŸ“‚ implementations (KÃ¼Ã§Ã¼k Ã§aplÄ± kodlar ve Ã¶rnek uygulamalar)

- `tokenization_example.py` â†’ Tokenization nasÄ±l Ã§alÄ±ÅŸÄ±r?
- `word_embedding_visualization.py` â†’ Embeddingâ€™leri gÃ¶rselleÅŸtirme
- `self_attention_demo.py` â†’ Self-Attention hesaplamasÄ±
- `transformer_from_scratch.py` â†’ Basit Transformer Modeli

### ğŸ“„ README.md&#x20;



